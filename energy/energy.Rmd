---
title: "Energy Conflict Data Cleaning and Tests"
author: "Alicia Peaker"
date: "3/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages
library(tidyverse)
library(lubridate) #to parse dates
library(tm)
library(dplyr)
library(ISOcodes)
library(tidytext)
library(scales)

# Load data
energy_data <- read_csv("~/projects/energyconflict/energy/energy_clean.csv")

#Set locale
#Sys.setlocale(locale = "de_DE.UTF-8")
```


```{r, include=FALSE}
#Clean up dates and newspaper names
parsed_date <- parse_date_time(energy_data$date_clean, "mdy")

#Sort by filename
energy_data <- energy_data[order(energy_data$filename),]

# Clean up newspaper names
energy_data <- energy_data %>% 
  mutate(newspaper = fct_recode(newspaper, "Süddeutsche Zeitung" = "s|ddeutsche.de", "Süddeutsche Zeitung" = "Sueddeutsche Zeitung", "Süddeutsche Zeitung" = "süddeutsche.de", "Süddeutsche Zeitung" = "Süddeutsche Zeitung (inkl. Regionalausgaben)")) 

```


# Number of articles, per newspaper, over time

```{r, echo=FALSE}
#Plot data

# Summarize dates by years and months
date_cleaned <- as.factor(energy_data$date_clean)

energy_data <- energy_data %>%
  mutate(years = year(date_clean), months = month(date_clean)) %>%
  group_by(years, months)
# don't forget to ungroup later!

#Render the plot
ggplot(energy_data,
       aes(x = years, fill=newspaper )) +
  geom_bar(position = "dodge")

```

# Instances of terms in dataset

```{r, include=FALSE, echo=FALSE}

#Process the text files as a corpus

# Read text files into a corpus called articles
articles <- VCorpus(DirSource("../energy_articles/all_clean", encoding = "UTF-8"),
                    readerControl = list(language = "german"))

#Make all text lowercase
articles <- tm_map(articles, content_transformer(tolower))
articles <- tm_map(articles, removePunctuation) 
articles <- tm_map(articles, removeWords, stopwords("de"))
articles <- tm_map(articles, stripWhitespace)
articles <- tm_map(articles, removeNumbers)


#had to finagle this list a little bit to get the right words for stemming
all_terms <- tolower(c("Atomkraftwerk", 
               "Demonstration", 
               "Jahrestag", 
               "Kalkar",
               "Kernkraftwerk",
               "Mahnwach",
               "Schneller Brüter",
              "Schneller Brueter",
              "WAA",
              "Wackersdorf",
              "Waldspaziergang",
              "Wiederaufarbeitungsanlag",
              "Wyhl"))

facilities <- tolower(c(
                "Atomkraftwerk",
                "Kernkraftwerk",
                "Schneller Brüter",
                "Schneller Brueter",
                "Wiederaufarbeitungsanlag"))

memorial <- tolower(c("Demonstration",
                      "Mahnwach",
                      "Jahrestag",
                      "Waldspaziergang"))

cities <- tolower(c("Wyhl",
                    "Wackersdorf",
                    "Kalkar"))

#There are two spellings of this in the corpus
bigrams <- tolower(c("Schneller Brüter",
                "Schneller Brueter"))


#Creates a bigram tokenizer that can also handle 1-word ngrams (1:2)
bigram_tokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
}

#Count the number of times each term above appears in each article in the corpus, includes appearances of word in title
terms_data_frame <- (DocumentTermMatrix(articles, control = list(dictionary = all_terms, stemming = TRUE)))

#Count the number of time each bigram appears in each article in the corpus, includes appearances of word in title
terms_data_frame2 <- (DocumentTermMatrix(articles, control = list(tokenizer = bigram_tokenizer, dictionary = bigrams)))

#inspect(terms_data_frame)

#Convert the dataframes into matrices and write to csv files
terms_data <- as.matrix(terms_data_frame)
terms_data2 <- as.matrix(terms_data_frame2)
write.csv(terms_data, "terms_data.csv")
write.csv(terms_data2, "terms_data2.csv")

#Load in new terms data matrix - necessary because it includes the observation id (filename) in the first column
terms <- read_csv("~/Documents/projects/energy/terms_data.csv")
terms2 <- read_csv("~/Documents/projects/energy/terms_data2.csv")

#Rename blank column (for filename observations) from original csv file 
names(terms)[names(terms) == "X1"] <- "filename" 
names(terms2)[names(terms2) == "X1"] <- "filename" 

#Join terms data with energy data by the filename column and omit duplicated columns without data (because initial processing of all_terms can't handle bigrams)
energy_data <- inner_join(energy_data, terms, by = "filename") %>% 
  ungroup()
energy_data <- select(energy_data, -c("schneller brueter","schneller brüter"))
energy_data <- inner_join(energy_data, terms2, by = "filename")

#Remove whitespace in column names
names(energy_data)<-str_replace_all(names(energy_data), c(" " = ""))

#Sum the two Schneller Brueter columns as a new column called "sb", and move them to the end (to make selecting column ranges easier in future functions)
energy_data <- energy_data %>% 
  mutate(sb = schnellerbrüter + schnellerbrueter) %>% 
  select(-schnellerbrüter,schnellerbrüter) %>% 
  select(-schnellerbrueter,schnellerbrueter)

```

# Instances of terms/categories in Wackersdorf, Bavaria published in the Süddeutsche Zeitung from 1994-2019


```{r, echo=FALSE}


#As of 3-3-2020, there are 288 articles published in Süddeutsche Zeitung that mention Wackersdorf (322 in total corpus)
bavaria <- filter(energy_data, wackersdorf > 0, newspaper == "Süddeutsche Zeitung") 

#Summarise if referring to facility or commemorative activity
bavaria_sum <- bavaria %>% 
  mutate(facilities = atomkraftwerk + kernkraftwerk + sb + wiederaufarbeitungsanlag + waa) %>% 
  mutate(memorial = demonstration + mahnwach + jahrestag + waldspaziergang) %>% 
  select(filename, title, date_clean, months, years, facilities, memorial)

bavaria_gather <- bavaria_sum %>% 
  pivot_longer(cols = facilities:memorial, names_to = 'type', values_to = 'count') %>% 
  select(date_clean, title, type, count) %>% 
  filter(count > 0) %>% 
  group_by(type, date=floor_date(date_clean, "month")) %>% 
  summarise(title = paste(title, collapse = " | "), count = sum(count))

#Uncomment to export the data
#bavaria_gather <- as.matrix(bavaria_gather)
#write.csv(bavaria_gather, "bavaria.csv")

ggplot(bavaria_gather, aes(x = date)) +
  geom_line(aes(y = count, color = type)) +
  # xlim(1994,1999) + //In case you want to look at a slice of time
  labs(title = "Wackersdorf, Bavaria",
       subtitle = "Mentions of nuclear facilities and commemoration in the Süddeutsche Zeitung from 1994-2019",
       y="Mentions",
       x="Date") +
  scale_x_date(date_breaks = "1 year", date_labels = "%b-%Y", expand = c(0,0), limits = as.Date(c("1994-01-01", "2020-01-01"))) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
        panel.grid.minor = element_blank())

  ggplot(data = bavaria_gather,
               aes(x = date, y = count, colour=type)) +
            geom_line(aes(linetype=type)) +
            scale_x_date(labels = date_format("%m-%Y")) +
            geom_point()

```
